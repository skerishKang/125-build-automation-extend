#!/usr/bin/env python3
"""
125 Build Automation - Telegram Bot Runner (Gemini 2.0 Flash Multimodal)
- Single file handling text/document/image/voice with Gemini 2.0 Flash
- Free chat with memory (Supabase optional)
- Document/Image/Voice processed directly with Gemini's multimodal capabilities
"""
import os
import sys
import logging
from datetime import datetime
from typing import Dict, List, Any
import tempfile
import asyncio

from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(__file__), ".env"))

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")

# logging
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join("logs", "bot_runner.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("unified_bot")

# Disable httpx logging to prevent token exposure
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("telegram").setLevel(logging.WARNING)

# telegram
try:
    from telegram import Update
    from telegram.constants import ChatAction
    from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
except ImportError:
    logger.error("python-telegram-bot is not installed. pip install python-telegram-bot==21.6")
    sys.exit(1)

# gemini (multimodal)
gemini_model = None
if GEMINI_API_KEY:
    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        logger.info("Using Gemini 2.5 Flash (multimodal)")
    except Exception as e:
        logger.error(f"Gemini setup failed: {e}")
else:
    logger.warning("GEMINI_API_KEY not set; chat will be disabled")

# supabase (optional memory)
supabase = None
if SUPABASE_URL and SUPABASE_KEY:
    try:
        from supabase import create_client
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    except Exception as e:
        logger.warning(f"Supabase init failed: {e}")

# in-memory recent docs (fallback)
recent_documents: Dict[int, List[Dict[str, Any]]] = {}

# Smart audio processing configuration
SHORT_AUDIO_THRESHOLD = int(os.getenv("SHORT_AUDIO_THRESHOLD", "30"))  # 30ì´ˆ ì´í•˜
LONG_AUDIO_THRESHOLD = int(os.getenv("LONG_AUDIO_THRESHOLD", "300"))  # 5ë¶„ ì´ìƒ
MID_LENGTH_MODEL = os.getenv("MID_LENGTH_AUDIO", "gemini")  # 30ì´ˆ-5ë¶„ ê¸°ë³¸


def get_audio_duration(ogg_path: str) -> float:
    """Get audio duration in seconds using ffprobe (if available) or estimate"""
    try:
        import subprocess
        result = subprocess.run(
            ["ffprobe", "-v", "quiet", "-show_entries", "format=duration",
             "-of", "csv=p=0", ogg_path],
            capture_output=True, text=True, check=True
        )
        return float(result.stdout.strip())
    except Exception:
        # Fallback: estimate based on file size (rough)
        # ~1MB per minute at 64kbps
        size_mb = os.path.getsize(ogg_path) / (1024 * 1024)
        return size_mb * 60 * 0.7  # conservative estimate


def format_plain(text: str, max_len: int = 1200) -> str:
    """Format Gemini response to Telegram-friendly plain text"""
    import re
    # Remove code blocks
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)
    # Remove tables
    text = re.sub(r"\|.*\|", "", text)
    # Remove header symbols (keep line breaks)
    text = re.sub(r"^#{1,6}\s*", "", text, flags=re.MULTILINE)
    # List symbols (keep line breaks)
    text = re.sub(r"^\s*[-*â€¢]\s*", "â€¢ ", text, flags=re.MULTILINE)
    text = re.sub(r"^\s*\d+\.\s*", "â€¢ ", text, flags=re.MULTILINE)
    # Remove bold/italic
    text = text.replace("**", "").replace("*", "")
    # Remove backticks
    text = text.replace("`", "'")
    # Clean up multiple line breaks (max 2)
    text = re.sub(r"\n{3,}", "\n\n", text)
    # Strip trailing spaces
    text = "\n".join(line.rstrip() for line in text.split("\n"))
    # Strip leading/trailing spaces
    text = text.strip()
    # Length limit with ...
    if len(text) > max_len:
        text = text[:max_len] + "â€¦"
    return text


# Thread pool for CPU-intensive operations (transcription, etc.)
from concurrent.futures import ThreadPoolExecutor
audio_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="audio_processing")


async def _action_indicator(context: ContextTypes.DEFAULT_TYPE, chat_id: int, action: str, stop_event: asyncio.Event):
    try:
        while not stop_event.is_set():
            try:
                await context.bot.send_chat_action(chat_id=chat_id, action=action)
            except Exception:
                pass
            # Telegramì€ 5ì´ˆ ë™ì•ˆ ì•¡ì…˜ ìœ ì§€. 4ì´ˆ ì£¼ê¸°ë¡œ ìƒˆë¡œ ì†¡ì‹ .
            try:
                await asyncio.wait_for(stop_event.wait(), timeout=4.0)
            except asyncio.TimeoutError:
                continue
    except Exception:
        pass

class ActionIndicator:
    def __init__(self, context: ContextTypes.DEFAULT_TYPE, chat_id: int, action: str):
        self.context = context
        self.chat_id = chat_id
        self.action = action
        self.stop_event = asyncio.Event()
        self.task: asyncio.Task | None = None

    async def __aenter__(self):
        self.task = asyncio.create_task(_action_indicator(self.context, self.chat_id, self.action, self.stop_event))
        return self

    async def __aexit__(self, exc_type, exc, tb):
        self.stop_event.set()
        if self.task:
            try:
                await asyncio.wait_for(self.task, timeout=1.5)
            except Exception:
                self.task.cancel()


async def save_memory(user_id: str, username: str, message: str, response: str):
    if not supabase:
        return
    try:
        supabase.table("conversations").insert({
            "user_id": user_id,
            "username": username,
            "message": message,
            "response": response,
            "created_at": datetime.utcnow().isoformat()
        }).execute()
    except Exception as e:
        logger.warning(f"save_memory failed: {e}")


async def fetch_memory(user_id: str, limit: int = 8) -> List[Dict[str, str]]:
    if not supabase:
        return []
    try:
        res = supabase.table("conversations").select("message,response,created_at").eq("user_id", user_id).order("created_at", desc=True).limit(limit).execute()
        return list(reversed(res.data or []))
    except Exception as e:
        logger.warning(f"fetch_memory failed: {e}")
        return []


async def reply_text(update: Update, text: str):
    # Prevent telegram 409: retry with slight delay on 409
    try:
        await update.message.reply_text(text)
    except Exception as e:
        logger.warning(f"reply_text failed: {e}")
        await asyncio.sleep(0.8)
        try:
            await update.message.reply_text(text[:4000])
        except Exception:
            pass


async def handle_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    name = update.effective_user.first_name or "ì‚¬ìš©ì"
    await reply_text(update,
        f"ì•ˆë…•í•˜ì„¸ìš” {name}ë‹˜! ğŸ‘‹\n\n"
        "ì´ ë´‡ì€ Gemini 2.5 Flash ê¸°ë°˜ \"ì˜¬ì¸ì›\"ì…ë‹ˆë‹¤.\n"
        "- ììœ  ëŒ€í™” (ë©”ëª¨ë¦¬ í¬í•¨)\n"
        "- ë¬¸ì„œ/ì´ë¯¸ì§€/ìŒì„± ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬\n\n"
        "ê·¸ëƒ¥ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê±°ë‚˜ íŒŒì¼ì„ ì˜¬ë ¤ë³´ì„¸ìš”.")


async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    if not text or text.startswith('/'):
        return

    if not GEMINI_API_KEY or not gemini_model:
        await reply_text(update, "Gemini ì„¤ì •ì´ ì—†ì–´ ëŒ€í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”.")
        return

    user_id = str(update.effective_user.id)
    username = update.effective_user.first_name or "ì‚¬ìš©ì"

    # Fetch memory and build context
    memory = await fetch_memory(user_id)
    context_lines = []
    if memory:
        context_lines.append("[ì´ì „ ëŒ€í™” ë§¥ë½]")
        for m in memory:
            context_lines.append(f"User: {m['message']}")
            context_lines.append(f"Assistant: {m['response']}")
        context_lines.append("")

    # Smart keyword detection for response length
    short_keywords = ["ìš”ì•½", "ê°„ë‹¨íˆ", "ì§§ê²Œ", "ìš”ì•½", "ê°„ë‹¨"]
    long_keywords = ["ìì„¸íˆ", "êµ¬ì²´ì ìœ¼ë¡œ", "ì„¤ëª…", "ìƒì„¸íˆ", "ìì„¸í•œ"]
    is_short_question = any(keyword in text for keyword in short_keywords)
    is_long_question = any(keyword in text for keyword in long_keywords)

    # Smart prompt
    if is_long_question:
        prompt_style = "ìì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
    elif is_short_question:
        prompt_style = "ê°„ë‹¨íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”."
    else:
        prompt_style = "ê°„ë‹¨íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”. ë” ìì„¸íˆ í•„ìš”í•˜ë©´ ì¶”ê°€ ìš”ì²­í•´ ì£¼ì„¸ìš”."

    prompt = "\n".join(context_lines + [
        f"í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: {text}",
        f"ë‹µë³€ ìŠ¤íƒ€ì¼: {prompt_style}",
        "í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , Markdown í‘œ/ì½”ë“œë¸”ë¡ ì—†ì´ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”."
    ])

    # 1) ì¦‰ì‹œ ìˆ˜ì‹  í™•ì¸ + ì•¡ì…˜ ì¸ë””ì¼€ì´í„° + ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ë£¨í”„ ì‹œì‘
    ack_msg = None
    try:
        ack_msg = await update.message.reply_text("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘â€¦ [0%]")
    except Exception:
        ack_msg = None

    indicator = ActionIndicator(context, update.effective_chat.id, ChatAction.TYPING)
    await indicator.__aenter__()

    progress_stop = asyncio.Event()

    async def progress_updater():
        if not ack_msg:
            return
        pct = 0
        try:
            while not progress_stop.is_set():
                pct = min(90, pct + 10)
                try:
                    await context.bot.edit_message_text(
                        chat_id=update.effective_chat.id,
                        message_id=ack_msg.message_id,
                        text=f"ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘â€¦ [{pct}%]"
                    )
                except Exception:
                    pass
                try:
                    await asyncio.wait_for(progress_stop.wait(), timeout=1.6)
                except asyncio.TimeoutError:
                    continue
        except Exception:
            pass

    progress_task = asyncio.create_task(progress_updater())

    try:
        # 2) ë¸”ë¡œí‚¹ ì¶”ë¡ ì„ ìŠ¤ë ˆë“œë¡œ ì˜¤í”„ë¡œë”©í•˜ì—¬ ë™ì‹œ ë©”ì‹œì§€ ì²˜ë¦¬ ìœ ì§€
        def _call_gemini():
            resp = gemini_model.generate_content(prompt)
            return resp.text.strip()
        raw = await asyncio.to_thread(_call_gemini)
        answer = format_plain(raw)
        logger.info(f"Bot replied ({len(answer)} chars): {answer[:100]}...")
    except Exception as e:
        logger.error(f"Gemini error: {e}")
        answer = "ì£„ì†¡í•´ìš”, ì§€ê¸ˆì€ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”."
    finally:
        # 3) ì§„í–‰ ë£¨í”„ ì¢…ë£Œ
        progress_stop.set()
        try:
            await asyncio.wait_for(progress_task, timeout=1.0)
        except Exception:
            progress_task.cancel()
        await indicator.__aexit__(None, None, None)

    # 4) ìµœì¢… 100%ë¡œ êµì²´ ë˜ëŠ” ìƒˆ ë©”ì‹œì§€ ì „ì†¡
    if ack_msg:
        try:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=ack_msg.message_id,
                text=f"âœ… ë‹µë³€ [100%]:\n{answer}"
            )
        except Exception:
            await reply_text(update, answer)
    else:
        await reply_text(update, answer)

    await save_memory(user_id, username, text, answer)


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    doc = update.message.document
    if not doc:
        return

    # Immediate acknowledgment to reduce perceived wait time
    ack_msg = None
    try:
        ack_msg = await update.message.reply_text("ğŸ“¥ íŒŒì¼ì„ ë°›ì•˜ì–´ìš”. ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤â€¦")
    except Exception:
        ack_msg = None

    file = await context.bot.get_file(doc.file_id)
    tmp = os.path.join(tempfile.gettempdir(), f"{doc.file_id}_{doc.file_name}")
    # ì—…ë¡œë“œ ì•¡ì…˜ ì¸ë””ì¼€ì´í„° ì‹œì‘
    doc_indicator = ActionIndicator(context, update.effective_chat.id, ChatAction.UPLOAD_DOCUMENT)
    await doc_indicator.__aenter__()
    await file.download_to_drive(tmp)

    # Only handle text files for now (simplified)
    try:
        content = open(tmp, 'rb').read()
        import chardet
        enc = chardet.detect(content).get('encoding') or 'utf-8'
        text = content.decode(enc, errors='ignore')
    except Exception as e:
        if ack_msg:
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=ack_msg.message_id,
                    text=f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}"
                )
            except Exception:
                pass
        else:
            await reply_text(update, f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return
    finally:
        try:
            os.remove(tmp)
        except Exception:
            pass

    if not GEMINI_API_KEY or not gemini_model:
        if ack_msg:
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=ack_msg.message_id,
                    text="Gemini ì„¤ì •ì´ ì—†ì–´ íŒŒì¼ ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”."
                )
            except Exception:
                pass
        else:
            await reply_text(update, "Gemini ì„¤ì •ì´ ì—†ì–´ íŒŒì¼ ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”.")
        await doc_indicator.__aexit__(None, None, None)
        return

    user_id = str(update.effective_user.id)
    username = update.effective_user.first_name or "ì‚¬ìš©ì"

    try:
        prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½/ë¶„ì„í•´ì¤˜. íŒŒì¼ëª…: {doc.file_name}\n\n{text}"
        prompt += "\n\ní•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , Markdown í‘œ/ì½”ë“œë¸”ë¡ ì—†ì´ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”."

        # Gemini call
        # ë¸”ë¡œí‚¹ ì¶”ë¡  ì˜¤í”„ë¡œë”©
        def _call_gemini_doc():
            resp = gemini_model.generate_content(prompt)
            return resp.text.strip()
        answer = await asyncio.to_thread(_call_gemini_doc)
        answer = format_plain(answer)
    except Exception as e:
        logger.error(f"Gemini doc error: {e}")
        answer = "ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”."

    # Update acknowledgment message or send new one
    final_text = f"ğŸ“„ {doc.file_name} ë¶„ì„ ê²°ê³¼:\n\n{answer}"
    if ack_msg:
        try:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=ack_msg.message_id,
                text=final_text
            )
        except Exception:
            await reply_text(update, final_text)
    else:
        await reply_text(update, final_text)

    recent_documents.setdefault(int(user_id), []).append({
        "file_name": doc.file_name,
        "text_length": len(text),
        "timestamp": datetime.utcnow()
    })
    await save_memory(user_id, username, f"[ë¬¸ì„œ] {doc.file_name}", answer)
    await doc_indicator.__aexit__(None, None, None)


async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not GEMINI_API_KEY or not gemini_model:
        await reply_text(update, "Gemini ì„¤ì •ì´ ì—†ì–´ ì´ë¯¸ì§€ ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”.")
        return

    # Immediate acknowledgment
    ack_msg = None
    try:
        ack_msg = await update.message.reply_text("ğŸ“· ì´ë¯¸ì§€ë¥¼ ë°›ì•˜ì–´ìš”. ë¶„ì„ ì¤‘â€¦")
    except Exception:
        ack_msg = None

    try:
        photo = update.message.photo[-1]
        file = await context.bot.get_file(photo.file_id)
        tmp = os.path.join(tempfile.gettempdir(), f"{photo.file_id}.jpg")
        photo_indicator = ActionIndicator(context, update.effective_chat.id, ChatAction.UPLOAD_PHOTO)
        await photo_indicator.__aenter__()
        await file.download_to_drive(tmp)

        # Step update: download complete
        if ack_msg:
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=ack_msg.message_id,
                    text="ğŸ“· ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì¤‘â€¦"
                )
            except Exception:
                pass

        # Use Gemini's multimodal capability - upload image directly
        import google.generativeai as genai
        image_part = {"mime_type": "image/jpeg", "data": open(tmp, "rb").read()}

        prompt = "ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ëŠ” ìº¡ì…˜ì„ ì‘ì„±í•´ì¤˜. ì´ë¯¸ì§€ì˜ ì£¼ìš” ë‚´ìš©, ìƒ‰ê°/ë¶„ìœ„ê¸°, ë§¥ë½ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        prompt += "\n\ní•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , Markdown í‘œ/ì½”ë“œë¸”ë¡ ì—†ì´ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”."

        # Multimodal call with image
        response = gemini_model.generate_content([prompt, image_part])
        answer = response.text.strip()
        answer = format_plain(answer)

        final_text = f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„¤ëª…:\n{answer}"
        if ack_msg:
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=ack_msg.message_id,
                    text=final_text
                )
            except Exception:
                await reply_text(update, final_text)
        else:
            await reply_text(update, final_text)
    except Exception as e:
        logger.error(f"photo error: {e}")
        if ack_msg:
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=ack_msg.message_id,
                    text="ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆì–´ìš”."
                )
            except Exception:
                await reply_text(update, "ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆì–´ìš”.")
        else:
            await reply_text(update, "ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆì–´ìš”.")
    finally:
        # Clean up temp file
        try:
            if 'tmp' in locals():
                os.remove(tmp)
        except Exception:
            pass
        try:
            if 'photo_indicator' in locals():
                await photo_indicator.__aexit__(None, None, None)
        except Exception:
            pass


async def handle_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not GEMINI_API_KEY or not gemini_model:
        await reply_text(update, "Gemini ì„¤ì •ì´ ì—†ì–´ ìŒì„± ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”.")
        return

    # Immediate acknowledgment + background processing message
    ack_msg = None
    try:
        ack_msg = await update.message.reply_text(
            "ğŸ¤ ìŒì„±ì„ ë°›ì•˜ì–´ìš”. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤! "
            "ë‹¤ë¥¸ ë©”ì‹œì§€ë„ ë°”ë¡œ ë³´ë‚¼ ìˆ˜ ìˆì–´ìš”. ğŸ˜Š"
        )
    except Exception:
        ack_msg = None

    chat_id = update.effective_chat.id
    user_id = str(update.effective_user.id)
    username = update.effective_user.first_name or "ì‚¬ìš©ì"

    # Create background task for voice processing (non-blocking)
    asyncio.create_task(process_voice_background(update, context, chat_id, user_id, username, ack_msg))


async def process_voice_background(update, context, chat_id, user_id, username, ack_msg):
    """Process voice in background - non-blocking, allows immediate responses"""
    voice = update.message.voice
    file = await context.bot.get_file(voice.file_id)
    ogg_path = os.path.join(tempfile.gettempdir(), f"{voice.file_id}.ogg")
    wav_path = os.path.join(tempfile.gettempdir(), f"{voice.file_id}.wav")

    try:
        # Download voice file
        await file.download_to_drive(ogg_path)

        # Get audio duration
        duration = get_audio_duration(ogg_path)

        # Select model based on duration
        if duration <= SHORT_AUDIO_THRESHOLD:
            # SHORT: Use Gemini 2.5 Flash (multimodal, fast)
            result = await process_with_gemini_multimodal(ogg_path, duration, chat_id, context, ack_msg)
            mode = "Gemini 2.5 Flash (ë©€í‹°ëª¨ë‹¬)"
        elif duration >= LONG_AUDIO_THRESHOLD:
            # LONG: Use Whisper + Gemini (accurate, free)
            result = await process_with_whisper_gemini(ogg_path, wav_path, duration, chat_id, context, ack_msg)
            mode = "Whisper + Gemini (ì •í™•ë„ ìµœì í™”)"
        else:
            # MID: Use environment setting
            if MID_LENGTH_MODEL == "gemini":
                result = await process_with_gemini_multimodal(ogg_path, duration, chat_id, context, ack_msg)
                mode = "Gemini 2.5 Flash (ë©€í‹°ëª¨ë‹¬)"
            else:
                result = await process_with_whisper_gemini(ogg_path, wav_path, duration, chat_id, context, ack_msg)
                mode = "Whisper + Gemini (ì •í™•ë„ ìµœì í™”)"

        # Send result
        if result:
            final_text = f"ğŸ¤ {mode} ì²˜ë¦¬ ê²°ê³¼ ({duration:.1f}ì´ˆ):\n\n{result}"
            if ack_msg:
                try:
                    await context.bot.edit_message_text(
                        chat_id=chat_id, message_id=ack_msg.message_id, text=final_text
                    )
                except Exception:
                    await context.bot.send_message(chat_id, final_text)
            else:
                await context.bot.send_message(chat_id, final_text)

            # Save to memory
            await save_memory(user_id, username, f"[ìŒì„±] {duration:.1f}ì´ˆ", result)

    except Exception as e:
        logger.error(f"Voice processing error: {e}")
        error_msg = f"ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)[:100]}"
        if ack_msg:
            try:
                await context.bot.edit_message_text(
                    chat_id=chat_id, message_id=ack_msg.message_id, text=error_msg
                )
            except Exception:
                await context.bot.send_message(chat_id, error_msg)
        else:
            await context.bot.send_message(chat_id, error_msg)
    finally:
        # Clean up
        try:
            for path in [ogg_path, wav_path]:
                if os.path.exists(path):
                    os.remove(path)
        except Exception:
            pass


async def process_with_gemini_multimodal(ogg_path: str, duration: float, chat_id: int, context, ack_msg):
    """Process short audio with Gemini 2.5 Flash multimodal"""
    # Update status
    if ack_msg:
        try:
            await context.bot.edit_message_text(
                chat_id=chat_id, message_id=ack_msg.message_id,
                text=f"ğŸ¤ {duration:.1f}ì´ˆ (ì§§ìŒ) - Gemini 2.5 Flash ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì¤‘..."
            )
        except Exception:
            pass

    # Upload audio directly to Gemini
    import google.generativeai as genai
    audio_data = open(ogg_path, "rb").read()
    audio_part = {"mime_type": "audio/ogg", "data": audio_data}

    prompt = (
        "ì´ ìŒì„± ë©”ì‹œì§€ë¥¼ í•œêµ­ì–´ë¡œ ì „ì‚¬í•˜ê³  ì ì ˆíˆ ìš”ì•½/ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        "ìŒì„± ë‚´ìš©ì— ì§ì ‘ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì´ë©´ ë‹µë³€ë„ ì œê³µí•´ì£¼ì„¸ìš”.\n"
        "í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , Markdown í‘œ/ì½”ë“œë¸”ë¡ ì—†ì´ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”."
    )

    # Call Gemini in thread pool
    def _call_gemini():
        response = gemini_model.generate_content([prompt, audio_part])
        return response.text.strip()

    result = await asyncio.to_thread(_call_gemini)
    return format_plain(result)


async def process_with_whisper_gemini(ogg_path: str, wav_path: str, duration: float, chat_id: int, context, ack_msg):
    """Process long audio with Whisper + Gemini"""
    # Update status
    if ack_msg:
        try:
            await context.bot.edit_message_text(
                chat_id=chat_id, message_id=ack_msg.message_id,
                text=f"ğŸ¤ {duration:.1f}ì´ˆ (ê¹€ìŒ) - Whisperë¡œ ì „ì‚¬ ì¤‘..."
            )
        except Exception:
            pass

    # Convert ogg to wav (async)
    try:
        proc = await asyncio.create_subprocess_exec(
            "ffmpeg", "-y", "-i", ogg_path, "-ar", "16000", "-ac", "1", wav_path,
            stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        _stdout, _stderr = await proc.communicate()
        if proc.returncode != 0:
            raise RuntimeError("ffmpeg ë³€í™˜ ì‹¤íŒ¨")
    except Exception as e:
        raise Exception(f"ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")

    # Update status
    if ack_msg:
        try:
            await context.bot.edit_message_text(
                chat_id=chat_id, message_id=ack_msg.message_id,
                text=f"ğŸ¤ ì „ì‚¬ ì™„ë£Œ! Geminië¡œ ìš”ì•½ ì¤‘..."
            )
        except Exception:
            pass

    # Whisper transcription (in thread pool)
    try:
        from faster_whisper import WhisperModel
        if not hasattr(process_with_whisper_gemini, "_whisper"):
            process_with_whisper_gemini._whisper = WhisperModel("base", device="cpu", compute_type="int8")
        wmodel = process_with_whisper_gemini._whisper

        def _transcribe():
            segs, _info = wmodel.transcribe(wav_path, language="ko", vad_filter=True)
            return " ".join([s.text.strip() for s in segs if s.text]).strip()

        transcription = await asyncio.to_thread(_transcribe)

        if not transcription:
            return "ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆì–´ìš”."

        # Gemini summary (in thread pool)
        def _summarize():
            prompt = (
                f"ë‹¤ìŒ ìŒì„± ë©”ì‹œì§€ê°€ ì „ì‚¬ëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì ì ˆíˆ ìš”ì•½í•˜ê±°ë‚˜ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n\n{transcription}\n\n"
                "í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , Markdown í‘œ/ì½”ë“œë¸”ë¡ ì—†ì´ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”."
            )
            response = gemini_model.generate_content(prompt)
            return response.text.strip()

        result = await asyncio.to_thread(_summarize)
        return format_plain(result)

    except ImportError:
        return "faster-whisperê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ìš”. ë°±ì—”ë“œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."



async def handle_list(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    docs = recent_documents.get(user_id, [])[-5:]
    if not docs:
        await reply_text(update, "ì €ì¥ëœ ìµœê·¼ ë¬¸ì„œê°€ ì—†ì–´ìš”.")
        return
    lines = [f"{i+1}. {d['file_name']} ({d['text_length']}ì)" for i, d in enumerate(docs)]
    await reply_text(update, "ìµœê·¼ ë¬¸ì„œ ëª©ë¡:\n" + "\n".join(lines))


def main():
    print("=== 125 Unified Telegram Bot (Gemini 2.5 Flash) ===")
    print(f"TELEGRAM_BOT_TOKEN: {'Set' if TELEGRAM_BOT_TOKEN else 'Not Found'}")
    print(f"GEMINI_API_KEY: {'Set' if GEMINI_API_KEY else 'Not Found'}")
    print(f"Supabase: {'Set' if (SUPABASE_URL and SUPABASE_KEY) else 'Not Set'}")

    if not TELEGRAM_BOT_TOKEN:
        print("ERROR: TELEGRAM_BOT_TOKEN is missing")
        return

    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", handle_start))
    app.add_handler(CommandHandler("list", handle_list))

    app.add_handler(MessageHandler(filters.Document.ALL, handle_document))
    app.add_handler(MessageHandler(filters.PHOTO, handle_photo))
    app.add_handler(MessageHandler(filters.VOICE, handle_voice))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))

    logger.info("Handlers registered. Starting polling...")
    app.run_polling()


if __name__ == "__main__":
    main()
