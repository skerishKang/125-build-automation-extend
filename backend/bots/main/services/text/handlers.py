"""Text-centric handlers extracted from the monolithic runtime."""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any, List

if TYPE_CHECKING:  # pragma: no cover - hints only
    from telegram import Update
    from telegram.ext import ContextTypes


async def handle_start(runtime: Any, update: "Update", context: "ContextTypes.DEFAULT_TYPE") -> None:
    """Greet the user and surface primary capabilities."""
    reply_text = runtime.reply_text
    ENABLE_DRIVE_MONITORING = runtime.ENABLE_DRIVE_MONITORING

    name = update.effective_user.first_name or "ì‚¬ìš©ì"
    monitoring_status = "ğŸ”„ Drive ìë™ ëª¨ë‹ˆí„°ë§" if ENABLE_DRIVE_MONITORING else "ğŸ“‹ Manual Drive ì²´í¬"
    await reply_text(
        update,
        (
            f"ì•ˆë…•í•˜ì„¸ìš” {name}ë‹˜! ğŸ‘‹\n\n"
            "ì´ ë´‡ì€ Gemini 2.5 Flash ê¸°ë°˜ \"ì˜¬ì¸ì›\"ì…ë‹ˆë‹¤.\n"
            "- ììœ  ëŒ€í™” (ë©”ëª¨ë¦¬ í¬í•¨)\n"
            "- ë¬¸ì„œ/ì´ë¯¸ì§€/ìŒì„± ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬\n"
            "- Google Drive ì–‘ë°©í–¥ ë™ê¸°í™”\n"
            "- Gmail ì‹¤ì‹œê°„ ê°ì‹œ ë° AI ìš”ì•½\n"
            f"- {monitoring_status}\n\n"
            "ğŸ“‚ **Drive ëª…ë ¹ì–´**: /drive\n"
            "ğŸ“§ **Gmail ëª…ë ¹ì–´**: /gmail_on, /gmail_off"
        ),
    )


async def handle_mode(runtime: Any, update: "Update", context: "ContextTypes.DEFAULT_TYPE") -> None:
    """Placeholder mode handler (kept for backward compatibility)."""
    reply_text = runtime.reply_text
    await reply_text(
        update,
        (
            "í˜„ì¬ëŠ” ê¸°ë³¸ ëŒ€í™” ëª¨ë“œë§Œ ì§€ì›í•©ë‹ˆë‹¤.\n"
            "í•„ìš”í•œ ëª¨ë“œê°€ ìˆë‹¤ë©´ ìš”ì²­í•´ ì£¼ì„¸ìš”!"
        ),
    )


async def handle_text(runtime: Any, update: "Update", context: "ContextTypes.DEFAULT_TYPE") -> None:
    """Main chat handler that routes user text to Gemini while using memory."""
    GEMINI_API_KEY = runtime.GEMINI_API_KEY
    gemini_chat_model = getattr(runtime, "gemini_chat_model", None)
    reply_text = runtime.reply_text
    fetch_memory = runtime.fetch_memory
    save_memory = runtime.save_memory
    ActionIndicator = runtime.ActionIndicator
    ChatAction = runtime.ChatAction
    format_plain = runtime.format_plain
    logger = runtime.logger

    text = (update.message.text or "").strip()
    if not text or text.startswith("/"):
        return

    if not GEMINI_API_KEY or not gemini_chat_model:
        await reply_text(update, "Gemini ì„¤ì •ì´ ì—†ì–´ ëŒ€í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”.")
        return

    user_id = str(update.effective_user.id)
    username = update.effective_user.first_name or "ì‚¬ìš©ì"

    memory: List[dict] = await fetch_memory(user_id)
    context_lines: List[str] = []
    if memory:
        context_lines.append("[ì´ì „ ëŒ€í™” ë§¥ë½]")
        for item in memory:
            context_lines.append(f"User: {item['message']}")
            context_lines.append(f"Assistant: {item['response']}")
        context_lines.append("")

    short_keywords = ["ìš”ì•½", "ê°„ë‹¨íˆ", "ì§§ê²Œ", "ìš”ì•½", "ê°„ë‹¨"]
    long_keywords = ["ìì„¸íˆ", "êµ¬ì²´ì ìœ¼ë¡œ", "ì„¤ëª…", "ìƒì„¸íˆ", "ìì„¸í•œ"]
    is_short_question = any(keyword in text for keyword in short_keywords)
    is_long_question = any(keyword in text for keyword in long_keywords)

    if is_long_question:
        prompt_style = "ìì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
    elif is_short_question:
        prompt_style = "ê°„ë‹¨íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”."
    else:
        prompt_style = "ê°„ë‹¨íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”. ë” ìì„¸íˆ í•„ìš”í•˜ë©´ ì¶”ê°€ ìš”ì²­í•´ ì£¼ì„¸ìš”."

    prompt = "\n".join(
        context_lines
        + [
            f"í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: {text}",
            f"ë‹µë³€ ìŠ¤íƒ€ì¼: {prompt_style}",
            "í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , Markdown í‘œ/ì½”ë“œë¸”ë¡ ì—†ì´ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.",
        ]
    )

    progress_messages = []
    progress_messages.append(await update.message.reply_text("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘â€¦ [10%]"))

    indicator = ActionIndicator(context, update.effective_chat.id, ChatAction.TYPING)
    await indicator.__aenter__()

    progress_messages.append(await update.message.reply_text("ğŸ§  Gemini 2.5 Flash-Lite ë¶„ì„ ì¤‘â€¦ [50%]"))

    try:
        def _call_gemini():
            response = gemini_chat_model.generate_content(prompt)
            return response.text.strip()

        raw = await asyncio.to_thread(_call_gemini)
        answer = format_plain(raw)
        logger.info("Bot replied (%s chars): %s...", len(answer), answer[:100])
    except Exception as exc:  # pragma: no cover - defensive logging
        logger.error("Gemini error: %s", exc)
        answer = "ì£„ì†¡í•´ìš”, ì§€ê¸ˆì€ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”."
    finally:
        await indicator.__aexit__(None, None, None)

    progress_messages.append(await update.message.reply_text("âœ… ë‹µë³€ ì™„ì„±! [100%]"))

    await reply_text(update, answer)
    await save_memory(user_id, username, text, answer)


async def handle_list(runtime: Any, update: "Update", context: "ContextTypes.DEFAULT_TYPE") -> None:
    """Show recent document history for the user."""
    reply_text = runtime.reply_text
    recent_documents = runtime.recent_documents

    user_id = update.effective_user.id
    docs = recent_documents.get(user_id, [])[-5:]
    if not docs:
        await reply_text(update, "ì €ì¥ëœ ìµœê·¼ ë¬¸ì„œê°€ ì—†ì–´ìš”.")
        return

    lines = [f"{index + 1}. {doc['file_name']} ({doc['text_length']}ì)" for index, doc in enumerate(docs)]
    await reply_text(update, "ìµœê·¼ ë¬¸ì„œ ëª©ë¡:\n" + "\n".join(lines))
